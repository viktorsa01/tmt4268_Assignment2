---
title: "Regression_Viktor"
output: html_document
date: "2023-04-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
set.seed(25)
library(GGally)
library(caret)
library(pROC)
```


```{r}
d.heart <- read.csv("heart.csv")

summary(d.heart)
#ggpairs(d.heart)
```

```{r}
training_set_size <- floor(0.8 * nrow(d.heart))
train_ind <- sample(seq_len(nrow(d.heart)), size = training_set_size)
train <- d.heart[train_ind, ]
test <- d.heart[-train_ind, ]

```

After some testing we excluded **RestingECG**, **MaxHR**, **Age**, and **RestingBP** from the analysis due to non-significant p-values when included in a model with all dependent variables. We then fitted the model to the training set and used a $0.5$ classification cutoff to predict the test set, generating a confusion matrix to evaluate performance.

```{r}

logRegModel <- glm(HeartDisease ~ . - RestingECG - MaxHR - Age - RestingBP, data = train, family = "binomial")


logRegPrediction <- predict(logRegModel, test, type="response")

logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)

logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified), 
                                         reference=as.factor(test$HeartDisease), 
                                         positive = '1')

print(logRegConfusionMatrix)

```

We get a misclassification error of $1 - 0.8587 = 0.1413$, sensitivity of $0.8899$ and specificity $0.8235$. As a rule of thumb the sum of specificity and sensitivity should be at least $1.5$, which means our model is performing quite well, though we might want to increase the sensitivity of the model. To get a better idea of what cutoff to chose we plot the ROC curve.

```{r}

logRegROC=roc(test$HeartDisease, logRegPrediction)

plot(logRegROC,main="ROC curve -- Logistic Regression")
```

From the ROC curve we find that $0.5$ seems to work well as a cutoff, but we might be able to increase the sensitivity without drastically decreasing performance. Therefore we try a cutoff of $0.3$. 

```{r}
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
#kanskje bruke cv for tuning? 

logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified), 
                                         reference=as.factor(test$HeartDisease), 
                                         positive = '1')

print(logRegConfusionMatrix)

```

We get misclassification error of $0.163$, a sensitivity of $0.939$ and specificity of $0.718$. Which we find to be a better result than the $0.5$ cutoff, due to the increase in sensitivity while slightly increasing the misclassification error. From the ROC curve we find that further increases in sensitivity will give diminishing returns while lowering the general performance.   

```{r}
logRegModel$coefficients
```
From the logistic regression model we have that: $$\frac{p_i}{1-p_i} = e^{\beta_0 + \beta_1x_{i1} + ... + \beta_nx_{in}}$$
$\frac{p_i}{1-p_i}$ is known as odds, and represents the ratio of a person getting heart disease and not getting heart disease. From the formula we can see that increasing $x_i$ by $1$, increases the odds by $e^{\beta_i}$. Therefore we get a picture of the impact of each variable on the response by looking coefficients, taking into account the scale of the corresponding variables. 

From the coefficients we find that **ST_Slope** and **ChestPainType** have the largest impact on the odds of heart disease. In particular people with a flat slope or asymptomatic chest pains have higher chances of heart disease. Otherwise we find that **SexM, FastingBS, ExerciseAnginaY** and **Oldpeak** all increase the odds, while **Cholesterol** decreases them slightly.



