---
title: "Compulsory Exercise 2: Title (give your project an informative title)"
author:
- Full name for group member \#1.
- Full name for group member \#2.
- Full name for group member \#3.
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output: 
  bookdown::html_document2
  # html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
  #  toc: no
  #  toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("knitr")
library("rmarkdown")
library(GGally)
library(caret)
library(pROC)
```

<!--  Etc (load all packages needed). -->

## Introduction: Scope and purpose of your project

## Descriptive data analysis/statistics

We load in the data set, and change the data type of all categorical variables to factor variables so that r knows the right encoding.

```{r}
heart <- read.csv("heart.csv")
heart$Sex <- factor(heart$Sex)
heart$ChestPainType <- factor(heart$ChestPainType)
heart$FastingBS <- factor(heart$FastingBS)
heart$RestingECG <- factor(heart$RestingECG)
heart$ExerciseAngina <- factor(heart$ExerciseAngina)
heart$ST_Slope <- factor(heart$ST_Slope)
heart$HeartDisease <- factor(heart$HeartDisease)
```

Then we run the **dim** and **summary** functions to get a quick overview of the data.

```{r}
dim (heart)
summary(heart)
```

We find that the data set contains 918 rows and and 12 columns. The twelve columns are: **Age, Sex, ChestPainType, RestingBP, Cholesterol, FastingBS, RestingECG, MaxHR, ExerciseAngina, Oldpeak, ST_Slope** and **HeartDisease**. ##Tror dette egentlig burde i introduction delen##. From the output of **summary** one can see the min, max, mean and median for all numerical variables and the distribution of the categorical variables. We notice that the data set contains significantly fewer women than men, which might skew the effect of **Sex** on the target variable. The same goes for **Age** as over half the people are between 45 and 60 years, which is about 25 % of the total age range of 28 to 77. 
 
To get a better view of the data we use the **ggpairs** function. 

```{r, fig.cap= "pairplot"}
ggpairs(heart)
```
From **figure XXX** we can se the distribution of all the variables, and we can see them plotted against each other. We also see the correlation between the numerical covariates, and notice that none of them seem to be highly correlated. **MaxHR** and **Age** have the largest value of - 0.382 which is indicates a moderate negative correlation between the two variables. 

## Methods

The classification methods we will be using are logistic regression, random forest and support vector machines. Logistic regression will be used for both prediction and inference, while random forest and support vector machines will be used **mainly/only** for prediction. We want to find out which model performs best, while also gaining insight into factors that indicate a higher risk of heart disease. 

The models will be evaluated by misclassification error, while also taking into account the sensitivity and specificity. Misclassification error is calculated by $\frac{incorrect predictions}{total predictions}$, meaning it is the portion of misclassified predictions and is therefore a clear indicator of the general performance of the model. Sensitivity in our case is the portion of people with heart disease correctly classified, while specificity is the portion of healthy people correctly classified. We will emphasize sensitivity, as false negatives will have larger consequences than false positives when trying to detect heart disease. This means we might trade some general performance, for increased sensitivity.   

Logistic regression is used for two-category classification problems like the one we are dealing with. We assume that the response variable $\mathbf{Y_i}$ follows a Bernoulli distribution with probability $p_i$, and that there is a linear relationship between the predictor variables $x_1,...,x_i$ and $\mathbf{Y_i}$. We link the predictor variables to the probability by the logistic link function: $$\log{\frac{p_i}{1-p_i}} = \beta_0 + \beta_1x_{i1} + ... + \beta_nx_{in}$$
Which is equivalent with: $$p_i = \frac{e^{\beta_0 + \beta_1x_{i1} + ... + \beta_nx_{in}}}{1+e^{\beta_0 + \beta_1x_{i1} + ... + \beta_nx_{in}}}$$
The coefficients $\beta_i$ are found by maximizing: $$L(\boldsymbol{\beta}) =  \prod_{i=1}^n (p_i)^{y_i}(1-p_i)^{1-y_i},$$
This is done automatically by the **glm** function in R. Each $p_i$ represents the probability that person $i$ is diagnosed with heart disease. We then chose a cutoff value of when predict **HeartDisease** to be ***TRUE**. The cutoff value will be tuned to optimize the model. Some upsides of logistic regression are that it is very simple to implement, in addition it is relatively easy to interpret and can be used for inference. Meaning we get insight into what are good indicators of heart disease. Downsides are poor performance if the relationship between response and predictors isn't linear and it is often outperformed by other methods for prediction. Logistic regression also requires no to average correlation between the independent variables, which shouldn't be an issue as we have seen from the descriptive analysis section. 
## Results and interpretation

## Summary
