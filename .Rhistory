print(logRegConfusionMatrix$byClass)
logRegROC=roc(test$HeartDisease, logRegPredictionClassified)
plot(logRegROC,main="ROC curve -- Logistic Regression")
logRegROC=roc(test$HeartDisease, logRegPrediction)
plot(logRegROC,main="ROC curve -- Logistic Regression")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix$byClass)
print(logRegConfusionMatrix$byClass)
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix$byClass)
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
table(logRegPredictionClassified)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix$table)
print(logRegConfusionMatrix$byClass)
logRegROC=roc(test$HeartDisease, logRegPrediction)
plot(logRegROC,main="ROC curve -- Logistic Regression")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix$byClass)
logRegModel$coefficients
d.heart <- read.csv("heart.csv")
training_set_size <- floor(0.8 * nrow(d.heart))
train_ind <- sample(seq_len(nrow(d.heart)), size = training_set_size)
train <- d.heart[train_ind, ]
test <- d.heart[-train_ind, ]
logRegModel <- glm(HeartDisease ~ . - HeartDisease, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
table(logRegPredictionClassified)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix$table)
print(logRegConfusionMatrix$byClass)
print(logRegConfusionMatrix)
??confusionMatrix
??confusionMatrix
print(logRegConfusionMatrix)
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix$byClass)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
set.seed(25)
library(GGally)
library(caret)
library(pROC)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
set.seed(25)
library(GGally)
library(caret)
library(pROC)
```{r}
d.heart <- read.csv("heart.csv")
training_set_size <- floor(0.8 * nrow(d.heart))
train_ind <- sample(seq_len(nrow(d.heart)), size = training_set_size)
train <- d.heart[train_ind, ]
test <- d.heart[-train_ind, ]
First we fit the model to the training set, and make predictions on the test set. Then we chose a cutoff of 0.5 for classification and make the corresponding confusion matrix.
```{r}
logRegModel <- glm(HeartDisease ~ . - HeartDisease, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
We get a misclassification error of $1 - 0.8641 = 0.1359$, sensitivity of $0.8791$ and specificity$0.8495$. As a rule of thumb the sum of specificity and sensitivity should be at least $1.5$, which means our model is performing quite well, though we might want to increase the sensitivity of the model. To get a better idea of what cutoff to chose we plot the ROC curve.
```{r}
logRegROC=roc(test$HeartDisease, logRegPrediction)
plot(logRegROC,main="ROC curve -- Logistic Regression")
plot(logRegROC,main="ROC curve -- Logistic Regression")
From the ROC curve we find that 0.5 seems to work well as a cutoff, but we might be able to increase the sensitivity without drastically decreasing performance. Therefore we try a cutoff of 0.3.
```{r}
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix$byClass)
print(logRegConfusionMatrix)
logRegPredictionClassified <- ifelse(logRegPrediction > 0.4, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegPredictionClassified <- ifelse(logRegPrediction > 0.35, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegPredictionClassified <- ifelse(logRegPrediction > 0.25, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegModel$coefficients
print(logRegROC$auc)
summary(d.heart)
ggpairs(heart)
heart <- read.csv("heart.csv")
heart$Sex <- factor(heart$Sex)
heart$ChestPainType <- factor(heart$ChestPainType)
heart$FastingBS <- factor(heart$FastingBS)
heart$RestingECG <- factor(heart$RestingECG)
heart$ExerciseAngina <- factor(heart$ExerciseAngina)
heart$ST_Slope <- factor(heart$ST_Slope)
heart$HeartDisease <- factor(heart$HeartDisease)
ggpairs(heart)
summary(d.heart)
logRegModel$coefficients
summary(logRegModel)
annova(logRegModel)
anova(logRegModel)
logRegModel <- glm(HeartDisease ~ . -age - RestingBP - RestingECG - Sex, data = train, family = "binomial")
logRegModel <- glm(HeartDisease ~ . -Age - RestingBP - RestingECG - Sex, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegModel <- glm(HeartDisease ~ . -Age - RestingBP - RestingECG, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegModel <- glm(HeartDisease ~ . -Age - RestingECG, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegModel <- glm(HeartDisease ~ . -Age - RestingECG, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegModel <- glm(HeartDisease ~ . - RestingECG, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegModel <- glm(HeartDisease ~ . , data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
set.seed(25)
library(GGally)
library(caret)
library(pROC)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
set.seed(25)
library(GGally)
library(caret)
library(pROC)
```{r}
d.heart <- read.csv("heart.csv")
summary(d.heart)
ggpairs(d.heart)
summary(d.heart)
ggpairs(d.heart)
```{r}
training_set_size <- floor(0.8 * nrow(d.heart))
train_ind <- sample(seq_len(nrow(d.heart)), size = training_set_size)
train <- d.heart[train_ind, ]
test <- d.heart[-train_ind, ]
First we fit the model to the training set, and make predictions on the test set. Then we chose a cutoff of 0.5 for classification and make the corresponding confusion matrix.
```{r}
logRegModel <- glm(HeartDisease ~ . , data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegModel <- glm(HeartDisease ~ . -HeartDisease, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
summary(logRegModel)
logRegModel <- glm(HeartDisease ~ . -RestingECG, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
heart <- read.csv("heart.csv")
heart$Sex <- factor(heart$Sex)
heart$ChestPainType <- factor(heart$ChestPainType)
heart$FastingBS <- factor(heart$FastingBS)
heart$RestingECG <- factor(heart$RestingECG)
heart$ExerciseAngina <- factor(heart$ExerciseAngina)
heart$ST_Slope <- factor(heart$ST_Slope)
heart$HeartDisease <- factor(heart$HeartDisease)
training_set_size <- floor(0.8 * nrow(d.heart))
train_ind <- sample(seq_len(nrow(d.heart)), size = training_set_size)
train <- d.heart[train_ind, ]
test <- d.heart[-train_ind, ]
training_set_size <- floor(0.8 * nrow(d.heart))
train_ind <- sample(seq_len(nrow(d.heart)), size = training_set_size)
train <- d.heart[train_ind, ]
test <- d.heart[-train_ind, ]
Then we run the **dim** and **summary** functions to get a quick overview of the data.
```{r}
ggpairs(heart)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
set.seed(25)
library(GGally)
library(caret)
library(pROC)
d.heart <- read.csv("heart.csv")
training_set_size <- floor(0.8 * nrow(d.heart))
train_ind <- sample(seq_len(nrow(d.heart)), size = training_set_size)
train <- d.heart[train_ind, ]
test <- d.heart[-train_ind, ]
logRegModel <- glm(HeartDisease ~ . , data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegROC=roc(test$HeartDisease, logRegPrediction)
plot(logRegROC,main="ROC curve -- Logistic Regression")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegModel$coefficients
summary(logRegModel)
anova(logRegModel)
summary(logRegModel)
logRegModel$coefficients
summary(d.heart)
rm(list=ls())
library(carData)
library(GGally)
library(ggplot2)
library(ggfortify)
summary(Salaries)
rm(list=ls())
library(carData)
library(GGally)
library(ggplot2)
library(ggfortify)
summary(Salaries)
```{r, fig.cap="Pairs plot of the Salaries dataset."}
ggpairs(Salaries)
ggpairs(Salaries)
a)
model1 <- lm(salary ~ ., data=Salaries)
summary(model1)
anova(model1)
---
title: "Regression_Viktor"
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
set.seed(25)
library(GGally)
library(caret)
library(pROC)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
set.seed(25)
library(GGally)
library(caret)
library(pROC)
```{r}
d.heart <- read.csv("heart.csv")
summary(d.heart)
training_set_size <- floor(0.8 * nrow(d.heart))
train_ind <- sample(seq_len(nrow(d.heart)), size = training_set_size)
train <- d.heart[train_ind, ]
test <- d.heart[-train_ind, ]
First we fit the model to the training set, and make predictions on the test set. Then we chose a cutoff of 0.5 for classification and make the corresponding confusion matrix.
```{r}
logRegModel <- glm(HeartDisease ~ . -RestingECG - , data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
logRegModel <- glm(HeartDisease ~ . , data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
We get a misclassification error of $1 - 0.8587 = 0.1413$, sensitivity of $0.8788$ and specificity$0.8353$. As a rule of thumb the sum of specificity and sensitivity should be at least $1.5$, which means our model is performing quite well, though we might want to increase the sensitivity of the model. To get a better idea of what cutoff to chose we plot the ROC curve.
```{r}
logRegROC=roc(test$HeartDisease, logRegPrediction)
plot(logRegROC,main="ROC curve -- Logistic Regression")
plot(logRegROC,main="ROC curve -- Logistic Regression")
From the ROC curve we find that 0.5 seems to work well as a cutoff, but we might be able to increase the sensitivity without drastically decreasing performance. Therefore we try a cutoff of 0.3.
```{r}
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
We get misclassification error of $0.1739$, a sensitivity of $0.919$ and specificity of $0.718$. Which we find to be a better result than the 0.5 cutoff, due to the increase in sensitivity while slightly increasing the misclassification error. From the ROC curve we find that further increases in sensitivity will give diminishing returns while lowering the general performance.
```{r}
logRegModel$coefficients
summary(logRegModel)
anova(logRegModel)
logRegModel$coefficients
summary(logRegModel)
anova(logRegModel)
```{r}
```{r}
summary(logRegModel)
logRegModel <- glm(HeartDisease ~ . - RestingECG - MacHR - Age - RestingBP, data = train, family = "binomial")
logRegModel <- glm(HeartDisease ~ . - RestingECG - MacHR - Age - RestingBP, data = train, family = "binomial")
logRegModel <- glm(HeartDisease ~ . - RestingECG - MaxHR - Age - RestingBP, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegModel <- glm(HeartDisease ~ . - RestingECG - MaxHR - Age - RestingBP, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegROC=roc(test$HeartDisease, logRegPrediction)
plot(logRegROC,main="ROC curve -- Logistic Regression")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
summary(d.heart)
logRegModel <- glm(HeartDisease ~ . - RestingECG - MaxHR - Age - RestingBP, data = train, family = "binomial")
logRegPrediction <- predict(logRegModel, test, type="response")
logRegPredictionClassified <- ifelse(logRegPrediction > 0.5, 1, 0)
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
logRegROC=roc(test$HeartDisease, logRegPrediction)
plot(logRegROC,main="ROC curve -- Logistic Regression")
plot(logRegROC,main="ROC curve -- Logistic Regression")
From the ROC curve we find that 0.5 seems to work well as a cutoff, but we might be able to increase the sensitivity without drastically decreasing performance. Therefore we try a cutoff of 0.3.
```{r}
logRegPredictionClassified <- ifelse(logRegPrediction > 0.3, 1, 0) #0.2 gir sens: 0.96, spec:0.61
logRegConfusionMatrix <- confusionMatrix(data=as.factor(logRegPredictionClassified),
reference=as.factor(test$HeartDisease),
positive = '1')
print(logRegConfusionMatrix)
We get misclassification error of $0.163$, a sensitivity of $0.939$ and specificity of $0.718$. Which we find to be a better result than the 0.5 cutoff, due to the increase in sensitivity while slightly increasing the misclassification error. From the ROC curve we find that further increases in sensitivity will give diminishing returns while lowering the general performance.
logRegModel$coefficients
summary(logRegModel)$p
summary(logRegModel$coefficients)
summary(logRegModel$effects)
summary(heart)
heart <- read.csv("heart.csv")
heart$Sex <- factor(heart$Sex)
heart$ChestPainType <- factor(heart$ChestPainType)
heart$FastingBS <- factor(heart$FastingBS)
heart$RestingECG <- factor(heart$RestingECG)
heart$ExerciseAngina <- factor(heart$ExerciseAngina)
heart$ST_Slope <- factor(heart$ST_Slope)
heart$HeartDisease <- factor(heart$HeartDisease)
summary(heart)
ggpairs(heart, mapping = ggplot2::aes(color = cut),
upper = list(continuous = wrap("density", alpha = 0.5), combo = "box_no_facet"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")
```{r,eval=TRUE,echo=FALSE}
library("knitr")
library("rmarkdown")
library(GGally)
library(caret)
library(pROC)
library(randomForest)
library("knitr")
library("rmarkdown")
library(GGally)
library(caret)
library(pROC)
library(randomForest)
<!--  Etc (load all packages needed). -->
## Introduction: Scope and purpose of your project
## Descriptive data analysis/statistics
We start by loading the data set, and changing the data type of all categorical variables to factor variables so that r knows the right encoding. Then we divide the data into a training and a test set, by a 80/20 split.
```{r}
heart <- read.csv("heart.csv")
heart$Sex <- factor(heart$Sex)
heart$ChestPainType <- factor(heart$ChestPainType)
heart$FastingBS <- factor(heart$FastingBS)
heart$RestingECG <- factor(heart$RestingECG)
heart$ExerciseAngina <- factor(heart$ExerciseAngina)
heart$ST_Slope <- factor(heart$ST_Slope)
heart$HeartDisease <- factor(heart$HeartDisease)
training_set_size <- floor(0.8 * nrow(heart))
train_ind <- sample(seq_len(nrow(heart)), size = training_set_size)
train <- heart[train_ind, ]
test <- heart[-train_ind, ]
training_set_size <- floor(0.8 * nrow(heart))
train_ind <- sample(seq_len(nrow(heart)), size = training_set_size)
train <- heart[train_ind, ]
test <- heart[-train_ind, ]
Then we run the $\texttt{dim()}$ and $\texttt{summary())}$ functions to get a quick overview of the data.
```{r}
dim (heart)
summary(heart)
dim (heart)
summary(heart)
We find that the data set contains 918 rows and and 12 columns. The twelve columns are: **Age, Sex, ChestPainType, RestingBP, Cholesterol, FastingBS, RestingECG, MaxHR, ExerciseAngina, Oldpeak, ST_Slope** and **HeartDisease**. ##Tror dette egentlig burde i introduction delen##. From the output of **summary** one can see the min, max, mean and median for all numerical variables and the distribution of the categorical variables. We notice that the data set contains significantly fewer women than men, which might skew the effect of **Sex** on the target variable. The same goes for **Age** as over half the people are between 45 and 60 years, which is about 25 % of the total age range of 28 to 77.
To get a better view of the data we use the $\texttt{ggpairs()}$ function.
```{r, fig.cap= "pairplot"}
ggpairs(heart, mapping = ggplot2::aes(color = cut),
upper = list(continuous = wrap("density", alpha = 0.5), combo = "box_no_facet"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
library(ggplot2)
ggpairs(heart, mapping = ggplot2::aes(color = cut),
upper = list(continuous = wrap("density", alpha = 0.5), combo = "box_no_facet"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(color = blue),
upper = list(continuous = wrap("density", alpha = 0.5), combo = "box_no_facet"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(color = "cut"),
upper = list(continuous = wrap("density", alpha = 0.5), combo = "box_no_facet"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(color = "blue"),
upper = list(continuous = wrap("density", alpha = 0.5), combo = "box_no_facet"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(color = "blue"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart),
ggpairs(heart,
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(color = "blue"),
combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(color = "blue"),
combo = wrap("dot_no_facet", alpha = 0.4))
ggpairs(heart, mapping = ggplot2::aes(color = Age),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(color = Sex),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(color = HeartDisease),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4)))
ggpairs(heart, mapping = ggplot2::aes(color = HeartDisease),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4))) + theme(axis.text = element_text(size = 5))
ggpairs(heart, mapping = ggplot2::aes(color = HeartDisease),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4))) + theme(axis.text = element_text(size = 8))
ggpairs(heart, mapping = ggplot2::aes(color = HeartDisease),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4))) + theme(axis.text = element_text(size = 7))
ggpairs(heart, mapping = ggplot2::aes(color = HeartDisease),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4))) + theme(axis.text = element_text(size = 6))
dev.new(width = 20, height = 10, unit = "in")
ggpairs(heart, mapping = ggplot2::aes(color = HeartDisease),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4))) + theme(axis.text = element_text(size = 5))
ggpairs(heart, mapping = ggplot2::aes(color = "red"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4))) + theme(axis.text = element_text(size = 5))
ggpairs(heart, mapping = ggplot2::aes(color = "HeartDisease"),
upper = list(continuous = wrap("density", alpha = 0.5), combo = "box_no_facet"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4))) + theme(axis.text = element_text(size = 5))
ggpairs(heart, mapping = ggplot2::aes(color = HeartDisease),
upper = list(continuous = wrap("density", alpha = 0.5), combo = "box_no_facet"),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4))) + theme(axis.text = element_text(size = 5))
cor(heart)
ggpairs(heart, mapping = ggplot2::aes(color = HeartDisease),
lower = list(continuous = wrap("points", alpha = 0.3), combo = wrap("dot_no_facet", alpha = 0.4))) + theme(axis.text = element_text(size = 5))
dim (heart)
