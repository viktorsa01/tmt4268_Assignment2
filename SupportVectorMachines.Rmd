# Support Vector Machines

The Support Vector Machine (SVM) is a supervised learning algorithm that separates data points into different classes based on a decision boundary. The decision boundary separates the data points using a set of given predictor variables and a categorical response variable. Points that lie on one side of the boundary are classified into one category, while points that lie on the other side are categorized into another.

The decision boundary is optimized with respect to the margin, which is the distance between the decision boundary and the closest data points from each separate class. The data points that lie the closest to the decision boundary (which are the ones that contribute to making the decision boundary) are called the support vectors. The number of support vectors may vary depending on the hyperparameter C, normally referred to as the "cost". Essentially what the cost hyperparameter does is control the bias-variance trade-off of the specific model fit. A high cost leads to a narrower margin and potentially overfitting (low bias, high variance), and a low cost results in a wider margin and potentially underfitting (high bias, low variance).

The decision boundary can either be linear or nonlinear, and the shape of the model depends on the choice of the so-called kernel. The choice of kernel, in combination with values for the hyperparameters, results in a specific SVM model fit, and can be tuned using cross-validation.

In the following analysis we will discuss **1)** how the choice of model affects the predictive ability of our model and **2)** how the choice of hyper-parameters (e.g. cost, gamma) affects the classifications rate.

## Implementation

First, we begin by loading the appropriate libraries and formatting the dataset properly.

```{r}
rm(list=ls())
library(ggplot2)
library(GGally)
library(e1071)
library(Hmisc)
library(ROCR)

heart = read.csv("heart.csv")
heart[, sapply(heart, is.character)] <- lapply(heart[, sapply(heart, is.character)], factor)
heart$HeartDisease = as.factor(heart$HeartDisease)

for (col in names(heart)) {
  if (is.character(heart[col])) {
    heart[col] <- as.factor(heart[col])
  }
}
heart$FastingBS = as.factor(heart$FastingBS)

heart$Cholesterol[heart$Cholesterol == 0] = NA
heart$Cholesterol = impute(heart$Cholesterol, median)

dim(heart)
#View(heart)
summary(heart[, 1:7])
summary(heart[, 8:12])
```

```{r}
set.seed(25)
train = sample(nrow(heart), nrow(heart)*0.8); test = -train
heart.train = heart[train, ]
heart.test  = heart[test, ]
```

```{r}
plot(heart.train$Age, heart.train$MaxHR, col=heart.train$HeartDisease)
```

```{r}
tune.optimal.linear = tune(svm, HeartDisease ~ ., data = heart.train, 
                kernel = "linear", 
                ranges = list(cost = c(0.1, 1, 5, 10)
              ))
summary(tune.optimal.linear)
tune.optimal.radial = tune(svm, HeartDisease ~ ., data = heart.train, 
                kernel = "radial", 
                ranges = list(cost = c(0.1, 1, 5, 10), gamma = c(0.5, 1, 2, 10)
              ))
summary(tune.optimal.radial)
```

From tuning on the set of given hyperparameters, the optimal cost for the linear kernel is $0.1$ and the optimal cost and gamma for the radial kernel is 1 and 0.5, respectively.

```{r}
rocplot = function(pred, truth, ...) {
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
}
```

```{r}
svmfit = function(formula_svm, formula_plot, data.train, data.test, kernel, cost, gamma, plot) {
  svmfit = svm(formula = formula_svm, 
             data = data.train, 
             kernel = kernel,
             gamma = gamma,
             cost = cost,
             scale=TRUE,
             decision.values = TRUE)
  
  if (plot) {
    plot(svmfit, heart.train, formula_plot)
  }
  
  heart.pred = predict(svmfit, heart.test)
  table_data = table(predict = heart.pred, truth = data.test$HeartDisease)
  
  hyperparameters = sprintf("Cost: %.2f, gamma: %.2f ", cost, gamma)
  print(hyperparameters)
  print(table_data)
  
  sensitivity = table_data[1] / (table_data[1] + table_data[2])
  sens = sprintf("Sensitivity: %.2f", sensitivity); print(sens)
  specificity = table_data[4] / (table_data[4] + table_data[3])
  spec = sprintf("Specificity: %.2f", specificity); print(spec)

  return(svmfit)
}


svmfit.linear.optimal = svmfit(HeartDisease ~ ., Cholesterol ~ Age, heart.train, heart.test, "linear", 1, 0.5, FALSE)

svmfit.linear.flexible = svmfit(HeartDisease ~ ., Cholesterol ~ Age, heart.train, heart.test, "linear", 100, 0.5, FALSE)

svmfit.radial.optimal = svmfit(HeartDisease ~ ., MaxHR ~ Age, heart.train, heart.test, "radial", 1, 0.5, FALSE)

svmfit.radial.flexible = svmfit(HeartDisease ~ ., MaxHR ~ Age, heart.train, heart.test, "radial", 1000, 0.2, FALSE)
```

Qualitatively the results for both radial and linear kernel seem similar. Hence, we need a quantitative approach in order to determine which kernel works better.

## ROC-curves:

ROC curves (receiver operating characteristic curves), illustrate the relationship between the true positive rate (TPR) and false positive rate (FPR) for a binary classifier. In the context of the **Health** dataset we are using, the TPR represents the rate at which the model is able to correctly predict patients with heart disease, while the FPR represents the rate at which the model incorrectly predicts patients without heart disease, as having the disease. By plotting the TPR against the FPR at different classification thresholds, we can evaluate the performance of the classifier and choose the optimal threshold based on our priorities and the relative costs of errors in different contexts. The area under the ROC curve (normally referred to as the AUC, area under the curve) provides a single numerical measure of the classifier's performance, with higher values indicating better performance. Larger is better.

### ROC for training data:

```{r}
fitted.linear.optimal = attributes(
  predict(svmfit.linear.optimal, heart.train, decision.values = TRUE)
)$decision.values

fitted.linear.flexible = attributes(
  predict(svmfit.linear.flexible, heart.train, decision.values = TRUE)
)$decision.values

fitted.radial.optimal = attributes(
  predict(svmfit.radial.optimal, heart.train, decision.values = TRUE)
)$decision.values

fitted.radial.flexible = attributes(
  predict(svmfit.radial.flexible, heart.train, decision.values = TRUE)
)$decision.values

rocplot(-fitted.linear.optimal, heart[train, "HeartDisease"], main = "Training Data", col="black")
rocplot(-fitted.linear.flexible, heart[train, "HeartDisease"], add = TRUE, col="red")
rocplot(-fitted.radial.optimal, heart[train, "HeartDisease"], add=TRUE, col="blue")
rocplot(-fitted.radial.flexible, heart[train, "HeartDisease"], add = TRUE, col="green")
```

In the above plot we see the ROC-curve for two different models with respectively two different model fits (blue, green for radial kernel, black, red for linear kernel) . The ROC curves for the radial kernel seem to score extraordinarily well with the training set, and there are a few reasons for why this may be. The radial kernel would have lower bias towards non-linear relationships between covariates. This may lead to overfitting the training data and result in higher accuracy on the training set compared to the linear kernel. However, this may come at the expense of generalization performance on new, unseen data, as the model may have learned false patterns in the training set that really do not apply for the population.

It's important to note that the ROC curve is a diagnostic tool for evaluating the performance of a binary classifier and does not provide insight into the underlying causes of differences in performance between different models or kernel functions. Next, we illustrate this by applying the model on the test set.

### ROC for test data:

The ROC curves for the test data provide a more reliable assessment of the model's ability to accurately classify new, unseen data, and can help guide model selection and hyperparameter tuning.

```{r}
fitted.linear.optimal = attributes(
  predict(svmfit.linear.optimal, heart.test, decision.values = TRUE)
)$decision.values

fitted.linear.flexible = attributes(
  predict(svmfit.linear.flexible, heart.test, decision.values = TRUE)
)$decision.values

fitted.radial.optimal = attributes(
  predict(svmfit.radial.optimal, heart.test, decision.values = TRUE)
)$decision.values

fitted.radial.flexible = attributes(
  predict(svmfit.radial.flexible, heart.test, decision.values = TRUE)
)$decision.values

rocplot(-fitted.linear.optimal, heart[test, "HeartDisease"], main="Test Data", col="black")
rocplot(-fitted.linear.flexible, heart[test, "HeartDisease"], add = TRUE, col = "red")
rocplot(-fitted.radial.optimal, heart[test, "HeartDisease"], add=TRUE, col="blue")
rocplot(-fitted.radial.flexible, heart[test, "HeartDisease"], add = TRUE, col = "green")
```

As we expected, the apparently outstanding results obtained by the radial kernels on the training set did not hold up nearly as well on the test set. Most noticeable is the green line (flexible radial kernel) that performed the best on the training set, but now performs the worst on the test set. This is a phenomenon is known as overfitting, and we see that the model has been fit too closely to the specific patterns in the training data and fails to generalize well on new, unseen data. In contrast the more conservative models have generalized well and indeed do
