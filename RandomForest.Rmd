---
title: "RandomForest"
author: "Lars Vatten"
date: "2023-04-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
rm(list=ls())
set.seed(25)
library(GGally)
library(randomForest)
library(caret)
library(ggplot2)
library(pROC)
```

Random forest is a method that can be used both for classification and regression. It is based on the idea of decision trees, but with some tweaks to obtain better accuracy. One of the main weaknesses of decision trees is high variance. To reduce variance, one can perform a method called *bagging*, which involves drawing multiple bootstrapped training sets from the data at hand, make a decision tree for each set, and then average (regression) or perform a majority vote (classification) to predict the value of an observation. This reduces variance since averaging a set of observations scales the variance by $\frac{1}{\text{# observations}}$. One could stop here, however, the procedure of producing multiple independent models introduces another problem. We recall that in a decision tree, each split is performed by choosing the predictor that best splits the data in two, measured by Gini index or entropy (our models are built using functions from the **randomForest** library, which finds the best split measured by Gini index). If we have few predictors, and some predictors have a relatively large p-value, they are at risk of being ignored in the model, since stronger predictors will be used to split the data inn all or most of the bagged trees. This will lead to a collection of trees where most of them are highly correlated through the strongest predictors. To solve this, instead of considering the full set of predictors at each split, we instead draw a random subset of predictors. This way, we ensure that all predictors will have their say in the prediction performed by the model, and avoid the scenario where one predictor determines the split of the root node in every tree. In total, these two tweaks results in vastly superior models in terms of prediction accuracy compared to normal decision trees. This, however, comes at the cost of interpretability, which is a major strength of simple trees.

In random forests, there are multiple of hyperparameters that can be optimized. We may choose the number of trees, maximum depth of each tree, number of predictors considered at each split, minimum number of observations at the leaves, etc. In this project we have decided to optimize only the number of trees and the number of predictors considered in each tree split. 

We create a random forest model using the $\texttt{randomForest()}$ function from the **randomForest** library:
```{r}
model <- randomForest(HeartDisease ~ ., data = train, importance = TRUE)
```
Perform prediction on unseen test data, and assess the prediction through a confusion matrix:
```{r}
prediction <- predict(model, test)
confusionMatrix <- confusionMatrix(data = prediction, reference = test$HeartDisease)
confusionMatrix$overall
```
We find that the model has a accuracy of of 0.8 **(something)**. Now we fiddle with the parameters of the model to see if it yields a better model.

First we investigate the number of trees in the forest. The number of trees is not a tuning parameter, it is just a question of whether we have enough trees for optimal classification, and of course we want as few trees as possible within this range to keep the complexity down. We plot the error rates for the positive, negative and Out-of-bag observations as functions of number of trees:
```{r}
plot(model$err.rate[,1], type="l", ylim = c(0.08, 0.22), ylab="Error rates", xlab = "Number of trees", col="blue")
lines(model$err.rate[,2], col="red")
lines(model$err.rate[,3], col="green")
legend(225, 0.225, legend=c("OOB", "Negatives", "Positives"), col=c("blue", "red", "green"), lty=1, cex=0.8)
```
The most interesting error is the OOB error, since this is the error rate measured on unseen data, and hence is a valid estimate of the test error. Observing the plot, it seems like all the error rates stabilize at around 100 trees, which means we can reduce complexity by reducing the number of trees from 500 to 100 without any loss of precision in the model.

Furthermore, we investigate the number of variables considered in each split in the trees. The default value in the $\texttt{randomForest()}$ function for classification problems corresponds to the floored square root of the number of predictors, in our case 3. The number of predictors considered can be seen as a tuning parameter, since we on one hand want to decorrelate the trees by allowing only a subset of the predictors to be considered, while we on the other hand need enough predictors to split the data properly and avoid bias. To find the optimal value, we test all possible number of predictors, i.e. 1 through 11. In each iteration, we store the OOB error rate at the final iteration.
```{r}
oobValues = vector(length=11)
for(i in 1:11) {
  newModel <- randomForest(HeartDisease ~ ., data = train, mtry=i, proximity = TRUE)
  oobValues[i] <- newModel$err.rate[nrow(newModel$err.rate), 1]
}
plot(oobValues, type = "o", xlab="Number of predictors considered", ylab="OOB error")
```
Observing the plot, the OOB error does vary depending on the number of predictors that are considered in each tree split. However, running the experiment multiple times shows that there is no unique optimal value, since the optimal value changes every iteration. It does seem like the model does not require many predictors to keep the OOB error down, which may be an indicator that the predictors are correlated and it is sufficent to only consider a few of them to split the data properly.

We create a new model with adjusted hyperparameters. We use $\texttt{ntree}=100$ and $\texttt{mtry}=2$.
```{r}
adjustedModel <- randomForest(HeartDisease ~ ., data = train, mtry = 2, ntree=100, importance = TRUE)

adjustedPrediction <- predict(adjustedModel, test)

confusionMatrix2 <- confusionMatrix(data = adjustedPrediction, reference = test$HeartDisease)
confusionMatrix2$overall
```

The accuracy of the model is 8. **(something)**. Another measure of the quality of the model is the AUC score, which we measure with the $\texttt{roc()}$ function from the **pRoc** library.

```{r}
probabilityPrediction <- predict(adjustedModel, test, type="prob")
ROC <- roc(test$HeartDisease, probabilityPrediction[,2])
ROC$auc
```

The model yields an AUC score of more than 0.9, which is good in almost all cases.

In addition to creating a predictive model, we want the model to say something about the relation between the response and the different covariates, i.e. inference. We use the $\texttt{varImpPlot()}$ function from the **randomForest** library, which approximates how separate predictors affects the accuracy of the model and the impurity of the root nodes in the trees of the model (measured by the Gini index).

```{r}
varImpPlot(adjustedModel)
```
Examining the plots, it seems like the **ST_Slope** and **ChestPainType** variables is most important for creating a good model, indicating that they have a strong correlation with the response variable. This is in agreement with the other logistic regression model.
